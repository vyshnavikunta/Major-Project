{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Only run this cell if you haven't configured Kaggle API in this Colab session before\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "def configure_kaggle_api_colab_local():\n",
        "    \"\"\"Configures the Kaggle API in Google Colab by uploading kaggle.json to the current directory.\"\"\"\n",
        "    print(\"üìÅ Please upload your kaggle.json file:\")\n",
        "    uploaded = files.upload()\n",
        "    if 'kaggle.json' in uploaded:\n",
        "        kaggle_json_path = \"kaggle.json\"\n",
        "        try:\n",
        "            with open(kaggle_json_path, \"wb\") as f:\n",
        "                f.write(uploaded['kaggle.json'])\n",
        "            os.chmod(kaggle_json_path, 0o600)\n",
        "            print(f\"‚úÖ kaggle.json saved to {kaggle_json_path}\")\n",
        "            print(\"üöÄ Kaggle API is now configured and ready to use!\")\n",
        "        except OSError as e:\n",
        "            print(f\"‚ö†Ô∏è Warning: Could not set file permissions for {kaggle_json_path}.\")\n",
        "            print(f\"Error: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è An unexpected error occurred while saving kaggle.json.\")\n",
        "            print(f\"Error: {e}\")\n",
        "    else:\n",
        "        print(\"‚ùå kaggle.json not uploaded.\")\n",
        "\n",
        "configure_kaggle_api_colab_local()\n",
        "\n",
        "# Set the environment variable to point to the local kaggle.json\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '.'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "_NAF4Cg2-hRH",
        "outputId": "e0f3df16-56bc-4c7f-8b37-56e3b55faaa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Please upload your kaggle.json file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-61e70c58-aae1-4171-855c-535d1f3ab6ac\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-61e70c58-aae1-4171-855c-535d1f3ab6ac\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "‚úÖ kaggle.json saved to kaggle.json\n",
            "üöÄ Kaggle API is now configured and ready to use!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = \"piyushsamant11/pidata-new-names\"\n",
        "zip_file_name = \"pidata-new-names.zip\"\n",
        "extract_to = \"dataset\"\n",
        "\n",
        "!kaggle datasets download -d {dataset_id} -p ./\n",
        "import zipfile\n",
        "import os\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "print(f\"‚úÖ Dataset extracted to: {extract_to}\")\n",
        "!rm {zip_file_name} # Clean up the zip file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4me4DQvp-ru_",
        "outputId": "4d8c0658-0e05-4990-cd47-ffdcee88d305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/piyushsamant11/pidata-new-names\n",
            "License(s): unknown\n",
            "Downloading pidata-new-names.zip to .\n",
            " 90% 576M/639M [00:03<00:01, 65.8MB/s]\n",
            "100% 639M/639M [00:03<00:00, 192MB/s] \n",
            "‚úÖ Dataset extracted to: dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms as T\n",
        "import random\n",
        "from skimage.measure import label, regionprops\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_and_preprocess_ct_scan(file_path):\n",
        "    try:\n",
        "        image = Image.open(file_path).convert('L')\n",
        "        image_array = np.array(image).astype(np.float32) / 255.0\n",
        "        return image_array\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading or preprocessing {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_mask_image(file_path):\n",
        "    try:\n",
        "        mask = Image.open(file_path).convert('L')\n",
        "        mask_array = np.array(mask) / 255.0\n",
        "        return mask_array.astype(bool)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading mask {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_patch(image, mask, bbox, patch_size=128):\n",
        "    x_min, y_min, x_max, y_max = map(int, bbox)\n",
        "    center_x = (x_min + x_max) // 2\n",
        "    center_y = (y_min + y_max) // 2\n",
        "    half_size = patch_size // 2\n",
        "    x1 = center_x - half_size\n",
        "    y1 = center_y - half_size\n",
        "    x2 = center_x + half_size\n",
        "    y2 = center_y + half_size\n",
        "\n",
        "    pad_left = max(0, -x1)\n",
        "    pad_top = max(0, -y1)\n",
        "    pad_right = max(0, x2 - image.shape[1])\n",
        "    pad_bottom = max(0, y2 - image.shape[0])\n",
        "\n",
        "    x1 = max(0, x1)\n",
        "    y1 = max(0, y1)\n",
        "    x2 = min(image.shape[1], x2)\n",
        "    y2 = min(image.shape[0], y2)\n",
        "\n",
        "    image_patch = image[y1:y2, x1:x2]\n",
        "    mask_patch = mask[y1:y2, x1:x2]\n",
        "\n",
        "    padded_image_patch = np.pad(image_patch, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant')\n",
        "    padded_mask_patch = np.pad(mask_patch, ((pad_top, pad_bottom), (pad_left, pad_right)), mode='constant')\n",
        "\n",
        "    return padded_image_patch, padded_mask_patch\n",
        "\n",
        "class ContrastiveLungNoduleDataset(Dataset):\n",
        "    def __init__(self, image_files, mask_dir, patch_size=128, num_positive_pairs=1, num_negative_samples=1, transform=None):\n",
        "        self.image_files = image_files\n",
        "        self.mask_dir = mask_dir\n",
        "        self.patch_size = patch_size\n",
        "        self.num_positive_pairs = num_positive_pairs\n",
        "        self.num_negative_samples = num_negative_samples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        image = load_and_preprocess_ct_scan(img_path)\n",
        "        mask = load_mask_image(os.path.join(self.mask_dir, os.path.splitext(os.path.basename(img_path))[0] + \".png\"))\n",
        "\n",
        "        if image is None or mask is None:\n",
        "            return None\n",
        "\n",
        "        labeled_mask = label(mask)\n",
        "        regions = regionprops(labeled_mask)\n",
        "        bboxes = [region.bbox for region in regions]\n",
        "\n",
        "        if not bboxes:\n",
        "            dummy_patch = np.zeros((self.patch_size, self.patch_size), dtype=np.float32)\n",
        "            dummy_patch_tensor = torch.from_numpy(dummy_patch).unsqueeze(0)\n",
        "            return dummy_patch_tensor, dummy_patch_tensor, dummy_patch_tensor\n",
        "\n",
        "        anchor_bbox = random.choice(bboxes)\n",
        "        anchor_image_patch, _ = extract_patch(image, mask, anchor_bbox, self.patch_size)\n",
        "        anchor_tensor = torch.from_numpy(anchor_image_patch).unsqueeze(0)\n",
        "        if self.transform:\n",
        "            anchor_tensor = self.transform(anchor_tensor)\n",
        "\n",
        "        positive_patches = []\n",
        "        for _ in range(self.num_positive_pairs):\n",
        "            positive_image_patch = self._augment(anchor_image_patch)\n",
        "            positive_tensor = positive_image_patch.unsqueeze(0)\n",
        "            if self.transform:\n",
        "                positive_tensor = self.transform(positive_tensor)\n",
        "            positive_patches.append(positive_tensor)\n",
        "\n",
        "        positive_tensor = torch.stack(positive_patches).squeeze(0).squeeze(0)\n",
        "\n",
        "        negative_patches = []\n",
        "        for _ in range(self.num_negative_samples):\n",
        "            negative_patch = self._create_negative_patch(image, mask, bboxes, anchor_bbox=anchor_bbox)\n",
        "            negative_tensor = torch.from_numpy(negative_patch).unsqueeze(0)\n",
        "            if self.transform:\n",
        "                negative_tensor = self.transform(negative_tensor)\n",
        "            negative_patches.append(negative_tensor)\n",
        "\n",
        "        negative_tensor = torch.stack(negative_patches).squeeze(1)\n",
        "\n",
        "        return anchor_tensor, positive_tensor, negative_tensor\n",
        "\n",
        "    def _create_negative_patch(self, image, mask, bboxes, anchor_bbox=None, healthy_ratio=0.5):\n",
        "        if random.random() < healthy_ratio or not bboxes or len(bboxes) < 2:\n",
        "            attempts = 0\n",
        "            while attempts < 10:\n",
        "                rand_y = random.randint(0, image.shape[0] - self.patch_size)\n",
        "                rand_x = random.randint(0, image.shape[1] - self.patch_size)\n",
        "                patch = mask[rand_y:rand_y + self.patch_size, rand_x:rand_x + self.patch_size]\n",
        "                if not np.any(patch):\n",
        "                    return image[rand_y:rand_y + self.patch_size, rand_x:rand_x + self.patch_size]\n",
        "                attempts += 1\n",
        "            rand_y = random.randint(0, image.shape[0] - self.patch_size)\n",
        "            rand_x = random.randint(0, image.shape[1] - self.patch_size)\n",
        "            return image[rand_y:rand_y + self.patch_size, rand_x:rand_x + self.patch_size]\n",
        "        else:\n",
        "            other_bboxes = [bbox for bbox in bboxes if bbox != anchor_bbox]\n",
        "            if other_bboxes:\n",
        "                other_bbox = random.choice(other_bboxes)\n",
        "                negative_image_patch, _ = extract_patch(image, mask, other_bbox, self.patch_size)\n",
        "                return negative_image_patch\n",
        "            else:\n",
        "                return self._create_negative_patch(image, mask, bboxes, healthy_ratio=1.0)\n",
        "\n",
        "    def _augment(self, image_patch):\n",
        "        transform = T.Compose([\n",
        "            T.RandomRotation(degrees=10),\n",
        "            T.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "            T.RandomAdjustSharpness(sharpness_factor=1.5, p=0.5),\n",
        "            T.RandomEqualize(p=0.5),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "        image_pil = Image.fromarray((image_patch * 255).astype(np.uint8))\n",
        "        return transform(image_pil)\n",
        "\n",
        "DATA_DIR = \"dataset/Dataset\"\n",
        "IMAGE_DIR = os.path.join(DATA_DIR, \"Images\")\n",
        "MASK_DIR = os.path.join(DATA_DIR, \"Annotations\")\n",
        "\n",
        "all_image_files = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if not f.startswith('.')]\n",
        "train_files, val_files = train_test_split(all_image_files, test_size=0.2, random_state=42)\n",
        "\n",
        "train_transform = T.Compose([\n",
        "    T.RandomRotation(degrees=15),\n",
        "    T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "    T.RandomAdjustSharpness(sharpness_factor=1.2, p=0.5),\n",
        "    T.RandomEqualize(p=0.3),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "val_transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "if os.path.exists(IMAGE_DIR) and os.path.exists(MASK_DIR):\n",
        "    contrastive_train_dataset = ContrastiveLungNoduleDataset(train_files, MASK_DIR, patch_size=128, num_positive_pairs=1, num_negative_samples=1, transform=train_transform)\n",
        "    contrastive_val_dataset = ContrastiveLungNoduleDataset(val_files, MASK_DIR, patch_size=128, num_positive_pairs=1, num_negative_samples=1, transform=val_transform)\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        batch = list(filter(lambda x: x is not None, batch))\n",
        "        if not batch:\n",
        "            return None\n",
        "        anchors = torch.stack([item[0] for item in batch])\n",
        "        positives = torch.stack([item[1] for item in batch])\n",
        "        negatives = torch.stack([item[2] for item in batch])\n",
        "        return anchors, positives, negatives\n",
        "\n",
        "    train_dataloader = DataLoader(contrastive_train_dataset, batch_size=32, shuffle=True, num_workers=2, collate_fn=collate_fn, drop_last=True)\n",
        "    val_dataloader = DataLoader(contrastive_val_dataset, batch_size=32, shuffle=False, num_workers=2, collate_fn=collate_fn, drop_last=False)\n",
        "\n",
        "    print(\"\\nContrastive Train dataloader created.\")\n",
        "    print(\"Contrastive Validation dataloader created.\")\n",
        "\n",
        "else:\n",
        "    print(\"Make sure the IMAGE_DIR and MASK_DIR paths are correct and exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syXKhHPt-vbG",
        "outputId": "1d693abf-94d6-486c-f5b8-440b027c8559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Contrastive Train dataloader created.\n",
            "Contrastive Validation dataloader created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "\n",
        "class VGG16Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = VGG16_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        vgg16 = models.vgg16(weights=weights)\n",
        "        self.features = nn.Sequential(*list(vgg16.features.children())[:-1])\n",
        "\n",
        "        first_conv_layer = self.features[0]\n",
        "        self.features[0] = nn.Conv2d(1, first_conv_layer.out_channels,\n",
        "                                            kernel_size=first_conv_layer.kernel_size,\n",
        "                                            stride=first_conv_layer.stride,\n",
        "                                            padding=first_conv_layer.padding)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.randn(1, 1, 128, 128)\n",
        "            output_features = self.features(dummy_input)\n",
        "            self.flattened_size = output_features.view(output_features.size(0), -1).shape[1]\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.fc = nn.Linear(self.flattened_size, self.embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Example instantiation (for checking)\n",
        "patch_size = 128\n",
        "embedding_dim = 128\n",
        "encoder = VGG16Encoder(embedding_dim, pretrained=True).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "dummy_patch = torch.randn(2, 1, patch_size, patch_size).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "embeddings = encoder(dummy_patch)\n",
        "print(\"Shape of embeddings (after correction):\", embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56m_E86E-xYj",
        "outputId": "7ba622c9-4f9f-42b5-c9e1-33075b5bd36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 528M/528M [00:06<00:00, 85.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings (after correction): torch.Size([2, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor, positive, negative):\n",
        "        dist_positive = F.pairwise_distance(anchor, positive)\n",
        "        dist_negative = F.pairwise_distance(anchor, negative)\n",
        "        losses = torch.relu(dist_positive - dist_negative + self.margin)\n",
        "        return torch.mean(losses)\n",
        "\n",
        "# Example usage (for checking)\n",
        "if __name__ == '__main__':\n",
        "    embedding_dim = 128\n",
        "    encoder = VGG16Encoder(embedding_dim, pretrained=True).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    triplet_loss_fn = TripletLoss(margin=1.0)\n",
        "    optimizer = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
        "\n",
        "    batch_size = 32\n",
        "    anchors = torch.randn(batch_size, 1, 128, 128).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    positives = torch.randn(batch_size, 1, 128, 128).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    negatives = torch.randn(batch_size, 1, 128, 128).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    anchor_embeddings = encoder(anchors)\n",
        "    positive_embeddings = encoder(positives)\n",
        "    negative_embeddings = encoder(negatives)\n",
        "\n",
        "    loss = triplet_loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"Triplet Loss:\", loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_jOx1sB-5ya",
        "outputId": "22fbe0aa-41b3-411d-a98a-f5750c14270b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Triplet Loss: 1.0108985900878906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "# --- Data Loading (re-define to ensure it's run in this context) ---\n",
        "DATA_DIR = \"dataset/Dataset\"\n",
        "IMAGE_DIR = os.path.join(DATA_DIR, \"Images\")\n",
        "MASK_DIR = os.path.join(DATA_DIR, \"Annotations\")\n",
        "\n",
        "all_image_files = [os.path.join(IMAGE_DIR, f) for f in os.listdir(IMAGE_DIR) if not f.startswith('.')]\n",
        "train_files, val_files = train_test_split(all_image_files, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = ContrastiveLungNoduleDataset(train_files, MASK_DIR, patch_size=128, num_positive_pairs=1, num_negative_samples=1)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "val_dataset = ContrastiveLungNoduleDataset(val_files, MASK_DIR, patch_size=128, num_positive_pairs=1, num_negative_samples=1)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=collate_fn, drop_last=False)\n",
        "\n",
        "# --- Model and Loss ---\n",
        "embedding_dim = 256\n",
        "encoder = VGG16Encoder(embedding_dim, pretrained=True).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "triplet_loss_fn = nn.TripletMarginLoss(margin=0.5).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optimizer = optim.Adam(encoder.parameters(), lr=0.0001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    encoder.train()\n",
        "    total_train_loss = 0.0\n",
        "    train_loader_tqdm = tqdm(train_dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", unit=\"batch\")\n",
        "    for batch_idx, batch in enumerate(train_loader_tqdm):\n",
        "        if batch is None:\n",
        "            continue\n",
        "        anchors, positives, negatives = batch\n",
        "        anchors = anchors.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        positives = positives.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        negatives = negatives.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        anchor_embeddings = encoder(anchors)\n",
        "        positive_embeddings = encoder(positives)\n",
        "        negative_embeddings = encoder(negatives)\n",
        "\n",
        "        loss = triplet_loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item() * anchors.size(0)\n",
        "        train_loader_tqdm.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Training Loss: {avg_train_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "    scheduler.step()\n",
        "\n",
        "    encoder.eval()\n",
        "    total_val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        val_loader_tqdm = tqdm(val_dataloader, desc=f\"Validation Epoch [{epoch+1}/{num_epochs}]\", unit=\"batch\")\n",
        "        for batch_idx, batch in enumerate(val_loader_tqdm):\n",
        "            if batch is None:\n",
        "                continue\n",
        "            anchors, positives, negatives = batch\n",
        "            anchors = anchors.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            positives = positives.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            negatives = negatives.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "            anchor_embeddings = encoder(anchors)\n",
        "            positive_embeddings = encoder(positives)\n",
        "            negative_embeddings = encoder(negatives)\n",
        "\n",
        "            loss = triplet_loss_fn(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
        "            total_val_loss += loss.item() * anchors.size(0)\n",
        "            val_loader_tqdm.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_dataloader.dataset)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "torch.save(encoder.state_dict(), 'contrastive_vgg16_encoder.pth')\n",
        "print(\"Trained contrastive encoder saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDnE2_4f-7pV",
        "outputId": "0707323a-926c-4317-d959-5df9da4f0ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [1/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:56<00:00,  1.12batch/s, loss=0.1122]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Average Training Loss: 0.1957, LR: 0.000100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [1/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10<00:00,  1.59batch/s, loss=0.0334]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Average Validation Loss: 0.0807\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [2/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:57<00:00,  1.09batch/s, loss=0.1094]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/20], Average Training Loss: 0.0680, LR: 0.000100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [2/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10<00:00,  1.46batch/s, loss=0.1164]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/20], Average Validation Loss: 0.0565\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [3/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.14batch/s, loss=0.1108]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/20], Average Training Loss: 0.0596, LR: 0.000100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [3/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10<00:00,  1.60batch/s, loss=0.0958]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/20], Average Validation Loss: 0.0466\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [4/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [01:00<00:00,  1.04batch/s, loss=0.0120]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/20], Average Training Loss: 0.0546, LR: 0.000100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [4/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10<00:00,  1.46batch/s, loss=0.0889]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/20], Average Validation Loss: 0.0536\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [5/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0755]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/20], Average Training Loss: 0.0507, LR: 0.000100\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [5/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.62batch/s, loss=0.0613]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/20], Average Validation Loss: 0.0466\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [6/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0318]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/20], Average Training Loss: 0.0419, LR: 0.000010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [6/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.60batch/s, loss=0.0034]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/20], Average Validation Loss: 0.0416\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [7/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0521]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/20], Average Training Loss: 0.0354, LR: 0.000010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [7/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.71batch/s, loss=0.0000]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/20], Average Validation Loss: 0.0361\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [8/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0146]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/20], Average Training Loss: 0.0321, LR: 0.000010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [8/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.63batch/s, loss=0.0647]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/20], Average Validation Loss: 0.0369\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [9/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:54<00:00,  1.15batch/s, loss=0.0251]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/20], Average Training Loss: 0.0347, LR: 0.000010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [9/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10<00:00,  1.53batch/s, loss=0.0807]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/20], Average Validation Loss: 0.0330\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [10/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0288]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/20], Average Training Loss: 0.0256, LR: 0.000010\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [10/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.63batch/s, loss=0.0824]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/20], Average Validation Loss: 0.0273\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [11/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:56<00:00,  1.11batch/s, loss=0.0360]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/20], Average Training Loss: 0.0298, LR: 0.000001\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [11/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.61batch/s, loss=0.0986]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/20], Average Validation Loss: 0.0389\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [12/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0370]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/20], Average Training Loss: 0.0333, LR: 0.000001\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [12/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.73batch/s, loss=0.0466]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/20], Average Validation Loss: 0.0386\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [13/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0320]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/20], Average Training Loss: 0.0340, LR: 0.000001\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation Epoch [13/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.64batch/s, loss=0.0295]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/20], Average Validation Loss: 0.0363\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [14/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:54<00:00,  1.15batch/s, loss=0.0513]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/20], Average Training Loss: 0.0292, LR: 0.000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [14/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.60batch/s, loss=0.0746]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/20], Average Validation Loss: 0.0370\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [15/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0604]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Average Training Loss: 0.0312, LR: 0.000001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [15/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10<00:00,  1.59batch/s, loss=0.0743]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/20], Average Validation Loss: 0.0326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [16/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.14batch/s, loss=0.0332]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Average Training Loss: 0.0330, LR: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [16/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.62batch/s, loss=0.0797]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/20], Average Validation Loss: 0.0340\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [17/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:56<00:00,  1.12batch/s, loss=0.0412]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Average Training Loss: 0.0339, LR: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [17/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10<00:00,  1.59batch/s, loss=0.1082]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/20], Average Validation Loss: 0.0388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [18/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.13batch/s, loss=0.0282]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Average Training Loss: 0.0269, LR: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [18/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.62batch/s, loss=0.0602]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/20], Average Validation Loss: 0.0319\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [19/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:55<00:00,  1.14batch/s, loss=0.0431]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Average Training Loss: 0.0285, LR: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [19/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:09<00:00,  1.61batch/s, loss=0.1070]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/20], Average Validation Loss: 0.0337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [20/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:56<00:00,  1.12batch/s, loss=0.0666]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Average Training Loss: 0.0325, LR: 0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [20/20]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:10<00:00,  1.50batch/s, loss=0.0961]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/20], Average Validation Loss: 0.0339\n",
            "Trained contrastive encoder saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class SegmentationLungNoduleDataset(Dataset):\n",
        "    def __init__(self, image_files, mask_dir, image_transform=None, mask_transform=None):\n",
        "        self.image_files = image_files\n",
        "        self.mask_dir = mask_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_files[idx]\n",
        "        mask_path = os.path.join(self.mask_dir, os.path.splitext(os.path.basename(img_path))[0] + \".png\")\n",
        "        image = load_and_preprocess_ct_scan(img_path)\n",
        "        mask = load_mask_image(mask_path).astype(np.float32)\n",
        "\n",
        "        if image is None or mask is None:\n",
        "            return None, None\n",
        "\n",
        "        image_pil = Image.fromarray((image * 255).astype(np.uint8))\n",
        "        mask_pil = Image.fromarray((mask * 255).astype(np.uint8))\n",
        "\n",
        "        image_tensor = image_pil\n",
        "        mask_tensor = mask_pil\n",
        "\n",
        "        if self.image_transform:\n",
        "            image_tensor = self.image_transform(image_tensor)\n",
        "        if self.mask_transform:\n",
        "            mask_tensor = self.mask_transform(mask_tensor)\n",
        "\n",
        "        return image_tensor, mask_tensor\n",
        "\n",
        "# Define separate transforms for images and masks\n",
        "image_segmentation_transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "mask_segmentation_transform = T.Compose([\n",
        "    T.Resize((256, 256)),\n",
        "    T.ToTensor()\n",
        "])\n",
        "\n",
        "# Assuming train_files, val_files, and MASK_DIR are defined\n",
        "segmentation_train_dataset = SegmentationLungNoduleDataset(train_files, MASK_DIR, image_transform=image_segmentation_transform, mask_transform=mask_segmentation_transform)\n",
        "segmentation_val_dataset = SegmentationLungNoduleDataset(val_files, MASK_DIR, image_transform=image_segmentation_transform, mask_transform=mask_segmentation_transform)\n",
        "\n",
        "def segmentation_collate_fn(batch):\n",
        "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
        "    if not batch:\n",
        "        return None, None\n",
        "    images = torch.stack([item[0] for item in batch])\n",
        "    masks = torch.stack([item[1] for item in batch])\n",
        "    return images, masks\n",
        "\n",
        "segmentation_train_loader = DataLoader(segmentation_train_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=segmentation_collate_fn, drop_last=True)\n",
        "segmentation_val_loader = DataLoader(segmentation_val_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=segmentation_collate_fn, drop_last=False)\n",
        "\n",
        "print(\"\\nSegmentation dataloaders created with separate transforms.\")"
      ],
      "metadata": {
        "id": "merEBmD3_aI5",
        "outputId": "45e4009a-643b-4ab2-b6b4-58b9a5ee2e87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Segmentation dataloaders created with separate transforms.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "\n",
        "class UNetWithFrozenEncoder(nn.Module):\n",
        "    def __init__(self, encoder, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder.features\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        vgg_channels = [64, 128, 256, 512] # 4 pooling layers\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(vgg_channels[-1], vgg_channels[-2], kernel_size=2, stride=2)\n",
        "        self.conv_decoder1 = nn.Conv2d(vgg_channels[-2] * 2, vgg_channels[-2], kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(vgg_channels[-2], vgg_channels[-3], kernel_size=2, stride=2)\n",
        "        self.conv_decoder2 = nn.Conv2d(vgg_channels[-3] * 2, vgg_channels[-3], kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(vgg_channels[-3], vgg_channels[-4], kernel_size=2, stride=2)\n",
        "        self.conv_decoder3 = nn.Conv2d(vgg_channels[-4] * 2, vgg_channels[-4], kernel_size=3, padding=1)\n",
        "\n",
        "        # Additional upsampling to reach 256x256\n",
        "        self.upconv_final = nn.ConvTranspose2d(vgg_channels[-4], vgg_channels[-4] // 2, kernel_size=2, stride=2)\n",
        "        self.conv_decoder_final = nn.Conv2d(vgg_channels[-4] // 2, vgg_channels[-4] // 2, kernel_size=3, padding=1)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(vgg_channels[-4] // 2, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder forward pass\n",
        "        pool_outputs = []\n",
        "        x = x\n",
        "        for i, layer in enumerate(self.encoder):\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, nn.MaxPool2d):\n",
        "                pool_outputs.append(x)\n",
        "\n",
        "        # Decoder forward pass with skip connections\n",
        "        d1 = self.upconv1(pool_outputs[-1])\n",
        "        d1 = torch.cat([d1, pool_outputs[-2]], dim=1)\n",
        "        d1 = F.relu(self.conv_decoder1(d1))\n",
        "\n",
        "        d2 = self.upconv2(d1)\n",
        "        d2 = torch.cat([d2, pool_outputs[-3]], dim=1)\n",
        "        d2 = F.relu(self.conv_decoder2(d2))\n",
        "\n",
        "        d3 = self.upconv3(d2)\n",
        "        d3 = torch.cat([d3, pool_outputs[-4]], dim=1)\n",
        "        d3 = F.relu(self.conv_decoder3(d3))\n",
        "\n",
        "        # Final upsampling\n",
        "        d_final_up = self.upconv_final(d3)\n",
        "        d_final = F.relu(self.conv_decoder_final(d_final_up))\n",
        "\n",
        "        output = torch.sigmoid(self.final_conv(d_final))\n",
        "        return output\n",
        "\n",
        "# Load the trained encoder weights\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "loaded_state_dict = torch.load('contrastive_vgg16_encoder.pth', map_location=device)\n",
        "\n",
        "loaded_encoder = VGG16Encoder(embedding_dim=512, pretrained=False)\n",
        "encoder_features_state_dict = loaded_encoder.features.state_dict()\n",
        "pretrained_features_state_dict = {}\n",
        "for name, param in loaded_state_dict.items():\n",
        "    if name.startswith('features.'):\n",
        "        pretrained_features_state_dict[name[len('features.'):]] = param\n",
        "\n",
        "encoder_features_state_dict.update(pretrained_features_state_dict)\n",
        "loaded_encoder.features.load_state_dict(encoder_features_state_dict)\n",
        "\n",
        "segmentation_model = UNetWithFrozenEncoder(loaded_encoder, num_classes=1).to(device)\n",
        "\n",
        "print(\"Segmentation model created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjbGIXBTOapO",
        "outputId": "c7f61153-0542-4a81-c295-616739376a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation model created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_segmentation = torch.optim.Adam(segmentation_model.parameters(), lr=0.001)\n",
        "num_epochs_segmentation = 25\n",
        "\n",
        "for epoch in range(num_epochs_segmentation):\n",
        "    segmentation_model.train()\n",
        "    total_loss = 0.0\n",
        "    train_loader_tqdm = tqdm(segmentation_train_loader, desc=f\"Segmentation Epoch [{epoch+1}/{num_epochs_segmentation}]\", unit=\"batch\")\n",
        "    for images, masks in train_loader_tqdm:\n",
        "        if images is None or masks is None:\n",
        "            continue\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer_segmentation.zero_grad()\n",
        "        outputs = segmentation_model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        loss.backward()\n",
        "        optimizer_segmentation.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        train_loader_tqdm.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(segmentation_train_loader.dataset)\n",
        "    print(f\"Segmentation Epoch [{epoch+1}/{num_epochs_segmentation}], Average Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    segmentation_model.eval()\n",
        "    dice_score = 0.0\n",
        "    num_batches = 0\n",
        "    with torch.no_grad():\n",
        "        val_loader_tqdm = tqdm(segmentation_val_loader, desc=f\"Validation Epoch [{epoch+1}/{num_epochs_segmentation}]\", unit=\"batch\")\n",
        "        for images, masks in val_loader_tqdm:\n",
        "            if images is None or masks is None:\n",
        "                continue\n",
        "            images = images.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            masks = masks.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            outputs = segmentation_model(images)\n",
        "            predicted_masks = (outputs > 0.5).float()\n",
        "\n",
        "            intersection = (predicted_masks * masks).sum(dim=[1, 2, 3])\n",
        "            union = predicted_masks.sum(dim=[1, 2, 3]) + masks.sum(dim=[1, 2, 3]) + 1e-7\n",
        "            batch_dice = (2 * intersection / union).mean()\n",
        "            dice_score += batch_dice.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_dice_score = dice_score / num_batches if num_batches > 0 else 0\n",
        "    print(f\"Segmentation Epoch [{epoch+1}/{num_epochs_segmentation}], Average Validation Dice Score: {avg_dice_score:.4f}\")\n",
        "\n",
        "print(\"Downstream segmentation training and evaluation complete.\")\n",
        "\n",
        "torch.save(segmentation_model.state_dict(), 'segmentation_unet.pth')\n",
        "print(\"Trained segmentation model weights saved to segmentation_unet.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APT7d1b0Oc_o",
        "outputId": "246d8ac1-614e-4a23-e166-31e2dad64abc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [1/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.34batch/s, loss=0.0288]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [1/25], Average Training Loss: 0.0893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [1/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.82batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [1/25], Average Validation Dice Score: 0.2891\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [2/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.33batch/s, loss=0.0215]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [2/25], Average Training Loss: 0.0280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [2/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.84batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [2/25], Average Validation Dice Score: 0.3021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [3/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.32batch/s, loss=0.0143]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [3/25], Average Training Loss: 0.0234\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [3/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.85batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [3/25], Average Validation Dice Score: 0.4696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [4/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.30batch/s, loss=0.0143]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [4/25], Average Training Loss: 0.0203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [4/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.71batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [4/25], Average Validation Dice Score: 0.5288\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [5/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.38batch/s, loss=0.0199]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [5/25], Average Training Loss: 0.0192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [5/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.88batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [5/25], Average Validation Dice Score: 0.5648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [6/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.36batch/s, loss=0.0176]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [6/25], Average Training Loss: 0.0172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [6/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.83batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [6/25], Average Validation Dice Score: 0.5035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [7/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.38batch/s, loss=0.0137]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [7/25], Average Training Loss: 0.0158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [7/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.74batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [7/25], Average Validation Dice Score: 0.5791\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [8/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.37batch/s, loss=0.0116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [8/25], Average Training Loss: 0.0167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [8/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.81batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [8/25], Average Validation Dice Score: 0.5884\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [9/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.36batch/s, loss=0.0191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [9/25], Average Training Loss: 0.0140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [9/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.88batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [9/25], Average Validation Dice Score: 0.5895\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [10/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.36batch/s, loss=0.0136]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [10/25], Average Training Loss: 0.0133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [10/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.83batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [10/25], Average Validation Dice Score: 0.6002\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [11/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.35batch/s, loss=0.0124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [11/25], Average Training Loss: 0.0127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [11/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.83batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [11/25], Average Validation Dice Score: 0.6119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [12/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.36batch/s, loss=0.0103]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [12/25], Average Training Loss: 0.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [12/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.80batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [12/25], Average Validation Dice Score: 0.6342\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [13/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.33batch/s, loss=0.0089]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [13/25], Average Training Loss: 0.0117\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [13/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.81batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [13/25], Average Validation Dice Score: 0.6483\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [14/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.35batch/s, loss=0.0126]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [14/25], Average Training Loss: 0.0111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [14/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.77batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [14/25], Average Validation Dice Score: 0.6261\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [15/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.34batch/s, loss=0.0133]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [15/25], Average Training Loss: 0.0112\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [15/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.81batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [15/25], Average Validation Dice Score: 0.6466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [16/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.33batch/s, loss=0.0137]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [16/25], Average Training Loss: 0.0108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [16/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.79batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [16/25], Average Validation Dice Score: 0.6190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [17/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.36batch/s, loss=0.0093]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [17/25], Average Training Loss: 0.0100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [17/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.81batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [17/25], Average Validation Dice Score: 0.6508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [18/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.35batch/s, loss=0.0120]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [18/25], Average Training Loss: 0.0098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [18/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.71batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [18/25], Average Validation Dice Score: 0.6732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [19/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.35batch/s, loss=0.0089]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [19/25], Average Training Loss: 0.0093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [19/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.82batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [19/25], Average Validation Dice Score: 0.6676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [20/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.35batch/s, loss=0.0077]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [20/25], Average Training Loss: 0.0088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [20/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.81batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [20/25], Average Validation Dice Score: 0.6630\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [21/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:53<00:00,  2.35batch/s, loss=0.0102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [21/25], Average Training Loss: 0.0086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [21/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.80batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [21/25], Average Validation Dice Score: 0.6772\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [22/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.31batch/s, loss=0.0124]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [22/25], Average Training Loss: 0.0085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [22/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.82batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [22/25], Average Validation Dice Score: 0.6656\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [23/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.32batch/s, loss=0.0071]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [23/25], Average Training Loss: 0.0086\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [23/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.79batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [23/25], Average Validation Dice Score: 0.6732\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [24/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.32batch/s, loss=0.0089]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [24/25], Average Training Loss: 0.0083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [24/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.77batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [24/25], Average Validation Dice Score: 0.6237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Segmentation Epoch [25/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 126/126 [00:54<00:00,  2.30batch/s, loss=0.0071]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [25/25], Average Training Loss: 0.0082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation Epoch [25/25]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:11<00:00,  2.77batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation Epoch [25/25], Average Validation Dice Score: 0.6719\n",
            "Downstream segmentation training and evaluation complete.\n",
            "Trained segmentation model weights saved to segmentation_unet.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "from skimage.measure import label, regionprops\n",
        "import os\n",
        "\n",
        "MASK_DIR = \"dataset/Dataset/Annotations\"\n",
        "\n",
        "def load_mask_image(file_path):\n",
        "    try:\n",
        "        mask = Image.open(file_path).convert('L')\n",
        "        mask_array = np.array(mask) / 255.0\n",
        "        return mask_array.astype(float)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading mask {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_lesion_distances(ct_image_path, segmentation_model, image_transform, threshold=0.5, pixel_spacing=(1.0, 1.0)):\n",
        "    original_image_pil = Image.open(ct_image_path).convert('L')\n",
        "    original_width, original_height = original_image_pil.size\n",
        "    original_image_array = np.array(original_image_pil)\n",
        "\n",
        "    image_tensor = image_transform(original_image_pil).unsqueeze(0).to(next(segmentation_model.parameters()).device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = segmentation_model(image_tensor)\n",
        "        predicted_mask_resized = (output > threshold).float().squeeze().cpu().numpy()\n",
        "\n",
        "    predicted_mask_original_size = np.array(Image.fromarray(predicted_mask_resized).resize((original_width, original_height), Image.NEAREST))\n",
        "\n",
        "    labeled_mask_original_size = label(predicted_mask_original_size)\n",
        "    regions_original_size = regionprops(labeled_mask_original_size)\n",
        "\n",
        "    lesion_info = []\n",
        "    for i, region in enumerate(regions_original_size):\n",
        "        minr, minc, maxr, maxc = region.bbox\n",
        "        area_pixels = region.area\n",
        "        centroid_row, centroid_col = region.centroid\n",
        "\n",
        "        lesion_info.append({\n",
        "            'id': i + 1,\n",
        "            'bbox': (minr, minc, maxr, maxc),\n",
        "            'area_pixels': area_pixels,\n",
        "            'centroid': (centroid_row, centroid_col)\n",
        "        })\n",
        "\n",
        "    distances = []\n",
        "    if len(lesion_info) >= 2:\n",
        "        for i in range(len(lesion_info)):\n",
        "            for j in range(i + 1, len(lesion_info)):\n",
        "                r1, c1 = lesion_info[i]['centroid']\n",
        "                r2, c2 = lesion_info[j]['centroid']\n",
        "                distance_pixels = np.sqrt((r2 - r1)**2 + (c2 - c1)**2)\n",
        "                distance_real_world = np.sqrt(((c2 - c1) * pixel_spacing[1])**2 + ((r2 - r1) * pixel_spacing[0])**2)\n",
        "                distances.append((lesion_info[i]['id'], lesion_info[j]['id'], distance_real_world))\n",
        "\n",
        "    return original_image_array, predicted_mask_original_size, lesion_info, distances"
      ],
      "metadata": {
        "id": "3PWPLM7SOjwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from skimage.measure import label, regionprops\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VGG16Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, pretrained=True):\n",
        "        super().__init__()\n",
        "        weights = VGG16_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        vgg16 = models.vgg16(weights=weights)\n",
        "        self.features = nn.Sequential(*list(vgg16.features.children())[:-1])\n",
        "\n",
        "        first_conv_layer = self.features[0]\n",
        "        self.features[0] = nn.Conv2d(1, first_conv_layer.out_channels,\n",
        "                                        kernel_size=first_conv_layer.kernel_size,\n",
        "                                        stride=first_conv_layer.stride,\n",
        "                                        padding=first_conv_layer.padding)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.randn(1, 1, 128, 128)\n",
        "            output_features = self.features(dummy_input)\n",
        "            self.flattened_size = output_features.view(output_features.size(0), -1).shape[1]\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.fc = nn.Linear(self.flattened_size, self.embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class UNetWithFrozenEncoder(nn.Module):\n",
        "    def __init__(self, encoder, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder.features\n",
        "        for param in self.encoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        vgg_channels = [64, 128, 256, 512] # 4 pooling layers\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(vgg_channels[-1], vgg_channels[-2], kernel_size=2, stride=2)\n",
        "        self.conv_decoder1 = nn.Conv2d(vgg_channels[-2] * 2, vgg_channels[-2], kernel_size=3, padding=1)\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(vgg_channels[-2], vgg_channels[-3], kernel_size=2, stride=2)\n",
        "        self.conv_decoder2 = nn.Conv2d(vgg_channels[-3] * 2, vgg_channels[-3], kernel_size=3, padding=1) # Added kernel_size=3\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(vgg_channels[-3], vgg_channels[-4], kernel_size=2, stride=2)\n",
        "        self.conv_decoder3 = nn.Conv2d(vgg_channels[-4] * 2, vgg_channels[-4], kernel_size=3, padding=1)\n",
        "\n",
        "        # Additional upsampling to reach 256x256\n",
        "        self.upconv_final = nn.ConvTranspose2d(vgg_channels[-4], vgg_channels[-4] // 2, kernel_size=2, stride=2)\n",
        "        self.conv_decoder_final = nn.Conv2d(vgg_channels[-4] // 2, vgg_channels[-4] // 2, kernel_size=3, padding=1)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(vgg_channels[-4] // 2, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder forward pass\n",
        "        pool_outputs = []\n",
        "        x = x\n",
        "        for i, layer in enumerate(self.encoder):\n",
        "            x = layer(x)\n",
        "            if isinstance(layer, nn.MaxPool2d):\n",
        "                pool_outputs.append(x)\n",
        "\n",
        "        # Decoder forward pass with skip connections\n",
        "        d1 = self.upconv1(pool_outputs[-1])\n",
        "        d1 = torch.cat([d1, pool_outputs[-2]], dim=1)\n",
        "        d1 = F.relu(self.conv_decoder1(d1))\n",
        "\n",
        "        d2 = self.upconv2(d1)\n",
        "        d2 = torch.cat([d2, pool_outputs[-3]], dim=1)\n",
        "        d2 = F.relu(self.conv_decoder2(d2))\n",
        "\n",
        "        d3 = self.upconv3(d2)\n",
        "        d3 = torch.cat([d3, pool_outputs[-4]], dim=1)\n",
        "        d3 = F.relu(self.conv_decoder3(d3))\n",
        "\n",
        "        # Final upsampling\n",
        "        d_final_up = self.upconv_final(d3)\n",
        "        d_final = F.relu(self.conv_decoder_final(d_final_up))\n",
        "\n",
        "        output = torch.sigmoid(self.final_conv(d_final))\n",
        "        return output\n",
        "\n",
        "# --- Load your trained segmentation model ---\n",
        "def load_segmentation_model(model_path, device):\n",
        "    embedding_dim = 512 # Adjust if your encoder's embedding dim was different\n",
        "    loaded_encoder = VGG16Encoder(embedding_dim=embedding_dim, pretrained=False)\n",
        "    segmentation_model = UNetWithFrozenEncoder(loaded_encoder, num_classes=1).to(device)\n",
        "    try:\n",
        "        segmentation_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    except FileNotFoundError:\n",
        "        st.error(f\"Error: Model weights not found at {model_path}\")\n",
        "        return None\n",
        "    except RuntimeError as e:\n",
        "        st.error(f\"Error loading state_dict: {e}\")\n",
        "        return None\n",
        "    segmentation_model.eval()\n",
        "    return segmentation_model\n",
        "\n",
        "# --- Preprocessing function ---\n",
        "def preprocess_image(image):\n",
        "    image = image.convert('L')  # Convert to grayscale\n",
        "    transform = T.Compose([\n",
        "        T.Resize((256, 256)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "    return transform(image).unsqueeze(0)\n",
        "\n",
        "# --- Function to get lesion distances (modified for Streamlit) ---\n",
        "def get_lesion_info_and_distances(image, model, pixel_spacing=(1.0, 1.0), threshold=0.5):\n",
        "    original_width, original_height = image.size\n",
        "    original_image_array = np.array(image.convert('L'))\n",
        "    image_tensor = preprocess_image(image).to(next(model.parameters()).device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image_tensor)\n",
        "        predicted_mask_resized = (output > threshold).float().squeeze().cpu().numpy()\n",
        "\n",
        "    predicted_mask_original_size = np.array(Image.fromarray(predicted_mask_resized).resize((original_width, original_height), Image.NEAREST))\n",
        "    labeled_mask_original_size = label(predicted_mask_original_size)\n",
        "    regions_original_size = regionprops(labeled_mask_original_size)\n",
        "\n",
        "    lesion_info = []\n",
        "    for i, region in enumerate(regions_original_size):\n",
        "        minr, minc, maxr, maxc = region.bbox\n",
        "        area_pixels = region.area\n",
        "        centroid_row, centroid_col = region.centroid\n",
        "        lesion_info.append({\n",
        "            'id': i + 1,\n",
        "            'bbox': (minr, minc, maxr, maxc),\n",
        "            'area_pixels': area_pixels,\n",
        "            'centroid': (centroid_row, centroid_col)\n",
        "        })\n",
        "\n",
        "    distances = []\n",
        "    if len(lesion_info) >= 2:\n",
        "        for i in range(len(lesion_info)):\n",
        "            for j in range(i + 1, len(lesion_info)):\n",
        "                r1, c1 = lesion_info[i]['centroid']\n",
        "                r2, c2 = lesion_info[j]['centroid']\n",
        "                distance_pixels = np.sqrt((r2 - r1)**2 + (c2 - c1)**2)\n",
        "                distance_real_world = np.sqrt(((c2 - c1) * pixel_spacing[1])**2 + ((r2 - r1) * pixel_spacing[0])**2)\n",
        "                distances.append((lesion_info[i]['id'], lesion_info[j]['id'], distance_real_world))\n",
        "\n",
        "    return original_image_array, predicted_mask_original_size, lesion_info, distances\n",
        "\n",
        "# --- Main Streamlit App ---\n",
        "def main():\n",
        "    st.title(\"Necrotic Lung Lesion Distance Measurement\")\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Upload a CT Image...\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "    pixel_spacing_x = st.number_input(\"Pixel Spacing (X)\", value=1.0)\n",
        "    pixel_spacing_y = st.number_input(\"Pixel Spacing (Y)\", value=1.0)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_path = 'segmentation_unet.pth'\n",
        "    model = load_segmentation_model(model_path, device)\n",
        "\n",
        "    if model is not None and uploaded_file is not None:\n",
        "        image = Image.open(uploaded_file)\n",
        "        original_image, predicted_mask, lesion_info, distances = get_lesion_info_and_distances(\n",
        "            image, model, pixel_spacing=(pixel_spacing_y, pixel_spacing_x)\n",
        "        )\n",
        "\n",
        "        st.subheader(\"Original CT Image\")\n",
        "        st.image(original_image, use_container_width=True)\n",
        "\n",
        "        st.subheader(\"Detected Lesions\")\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(original_image, cmap='gray')\n",
        "        ax.imshow(predicted_mask, cmap='viridis', alpha=0.5)\n",
        "        for lesion in lesion_info:\n",
        "            bbox = lesion['bbox']\n",
        "            area = lesion['area_pixels']\n",
        "            minr, minc, maxr, maxc = bbox\n",
        "            rect = patches.Rectangle((minc, minr), maxc - minc, maxr - minr, linewidth=1, edgecolor='lime', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            ax.text(minc, minr - 5, f\"ID: {lesion['id']}, Area: {area}\", color='lime', fontsize=8, ha='left', va='top')\n",
        "            centroid_row, centroid_col = lesion['centroid']\n",
        "            ax.plot(centroid_col, centroid_row, 'w+', markersize=5)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.subheader(\"Distances Between Lesions (mm)\")\n",
        "        if distances:\n",
        "            for dist in distances:\n",
        "                st.write(f\"Lesion {dist[0]} and Lesion {dist[1]}: {dist[2]:.2f} mm\")\n",
        "        else:\n",
        "            st.write(\"Less than two lesions detected.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28RA25U6OmSD",
        "outputId": "f9c08340-fe9f-4b5f-b1f2-d0551c10c0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok\n",
        "\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import subprocess\n",
        "import os\n",
        "import signal\n",
        "\n",
        "# Kill any existing ngrok processes\n",
        "ngrok.kill()\n",
        "\n",
        "# Authtoken (replace with your actual authtoken)\n",
        "NGROK_AUTH_TOKEN = \"2y30gIHEqcnc0ruOyTFIu5HCCSk_4nBSRxNU1jvKYyevg6jFh\" # Replace with your actual token\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "# Set Streamlit server address and port\n",
        "streamlit_address = \"0.0.0.0\"\n",
        "streamlit_port = 8501\n",
        "streamlit_command = [\n",
        "    \"streamlit\", \"run\",\n",
        "    \"--server.address\", streamlit_address,\n",
        "    \"--server.port\", str(streamlit_port),\n",
        "    \"app.py\"\n",
        "]\n",
        "\n",
        "# Start Streamlit app in the background\n",
        "streamlit_process = subprocess.Popen(\n",
        "    streamlit_command,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    preexec_fn=os.setsid # Create a new process group\n",
        ")\n",
        "\n",
        "print(f\"‚è≥ Starting Streamlit app on {streamlit_address}:{streamlit_port}...\")\n",
        "time.sleep(15) # Give Streamlit ample time to start\n",
        "\n",
        "# Check if Streamlit is running internally using curl\n",
        "internal_url = f\"http://localhost:{streamlit_port}\"\n",
        "try:\n",
        "    curl_process = subprocess.run(\n",
        "        [\"curl\", \"-s\", internal_url],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=10\n",
        "    )\n",
        "    if \"streamlit\" in curl_process.stdout.lower():\n",
        "        print(f\"‚úÖ Streamlit is responding at {internal_url}\")\n",
        "        try:\n",
        "            # Open a ngrok tunnel to the Streamlit app's port\n",
        "            public_url = ngrok.connect(streamlit_port, bind_tls=True).public_url\n",
        "            print(f\"üåç Your public Streamlit URL (HTTPS): {public_url}\")\n",
        "            print(\"üëÜ Click on the link above to open your web application in a new tab.\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error creating ngrok tunnel: {e}\")\n",
        "            print(\"Please check your ngrok authtoken and internet connection.\")\n",
        "    else:\n",
        "        error_output = streamlit_process.stderr.read().decode(\"utf-8\")\n",
        "        print(f\"‚ùå Streamlit did not start correctly. Error output:\\n{error_output}\")\n",
        "\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(f\"‚ùå Streamlit did not respond at {internal_url} within the timeout. It might have failed to start.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå Error: curl command not found. This is unexpected in Colab.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An unexpected error occurred while checking Streamlit: {e}\")\n",
        "finally:\n",
        "    # Clean up the Streamlit process if it's still running\n",
        "    if streamlit_process.poll() is None:\n",
        "        os.killpg(os.getpgid(streamlit_process.pid), signal.SIGTERM) # Terminate the process group\n",
        "        streamlit_process.wait()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FtTReqsOqUX",
        "outputId": "0a9e5755-465c-4cad-bf62-13e484d57c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.45.1-py3-none-any.whl.metadata (8.9 kB)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.9-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.41.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.45.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.9-py3-none-any.whl (25 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m114.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pyngrok, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 pyngrok-7.2.9 streamlit-1.45.1 watchdog-6.0.0\n",
            "‚è≥ Starting Streamlit app on 0.0.0.0:8501...\n",
            "‚úÖ Streamlit is responding at http://localhost:8501\n",
            "üåç Your public Streamlit URL (HTTPS): https://f5b6-35-204-195-167.ngrok-free.app\n",
            "üëÜ Click on the link above to open your web application in a new tab.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHZbOQfCfqMg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}